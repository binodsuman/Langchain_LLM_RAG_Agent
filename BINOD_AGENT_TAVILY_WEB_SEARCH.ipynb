{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "tavily_api_key = os.environ[\"TAVILY_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "chatModel = ChatMistralAI(\n",
    "    mistral_api_key=mistral_api_key,\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=1,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RAG - Import PDF\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./data/kafka_definite_guide.pdf')\n",
    "\n",
    "loaded_data = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binod/Library/Caches/pypoetry/virtualenvs/langchain-mistral-pEPNAQNE-py3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Insert PDF to Vector DB\n",
    "vectorstore = Chroma.from_documents(documents=loaded_data, embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 5 Search from Vector DB\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template to send, TOP 5 search from Vector DB for LLM as context and Question.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If context is not available then you reply based on your information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To call LLM \n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chatModel\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Apache Kafka is used for the following purposes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n",
      "1. **Messaging System**: Kafka behaves as a message queue, providing ordering guarantees on the messages it stores. This means it can be used to pass messages between different systems or components in a reliable and ordered manner.\n",
      "\n",
      "2. **Data Streaming**: Kafka is used for streaming data between systems. It allows for the integration of data from various sources and can handle high throughput and low latency.\n",
      "\n",
      "3. **Distributed Systems**: Kafka is useful in distributed systems for its ability to handle large volumes of data and provide robust data pipelines. It is designed to work in a distributed environment and provides features like replication and fault tolerance.\n",
      "\n",
      "4. **Real-time Analytics**: Kafka's ability to process and analyze data in real-time makes it a powerful tool for applications that require immediate insights and responses based on live data.\n",
      "\n",
      "5. **Log Aggregation**: Kafka can be used to aggregate logs from multiple sources, providing a centralized way to store and process log data.\n",
      "\n",
      "Overall, Kafka is a versatile tool used for building real-time data pipelines and streaming applications, providing reliable messaging, data integration, and real-time analytics capabilities."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Empty request\"}')\n"
     ]
    }
   ],
   "source": [
    "query= \"what is the use of Apache Kafka\"\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there is no information about a person named Binod Suman. Therefore, I cannot answer who Binod Suman is."
     ]
    }
   ],
   "source": [
    "query= \"Who is Binod Suman\"\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addig Taveli for web search in case, information not present in provided PDF\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "online_search_tool = [TavilySearchResults(tavily_api_key = tavily_api_key, max_results=5)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = chatModel.bind_tools(online_search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot find any information about Binod Suman in the provided context. Based on my general knowledge, I am also unable to provide any information about this individual."
     ]
    }
   ],
   "source": [
    "query= \"Who is Binod Suman\"\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to enhance template, to support online search tool.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If context is not available then use search online tavily tool:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"How is weather today in Bangalore?\"\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = llm_with_tools.invoke(\"How is weather today in Bangalore?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_retriever = TavilySearchAPIRetriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 20, 'source': './data/kafka_definite_guide.pdf'}, page_content='support for all the extra time writing, and long hours\\nrunning to clear his head, keeps him going.\\nRajini would like to thank her husband, Manjunath, and\\nson, T arun, for their unwavering support and\\nencouragement, for spending weekends reviewing the early\\ndrafts, and for always being there for her .\\nKrit shares his love and gratitude with his wife, Cecilia, and\\ntwo children, Lucas and Lizabeth. Their love and support\\nmake every day a joy , and he wouldn’t be able to pursue his\\npassions without them. He also wants to thank his mom,\\nCindy P etty, for instilling in Krit a desire to always be the\\nbest version of himself .'),\n",
       " Document(metadata={'page': 3, 'source': './data/kafka_definite_guide.pdf'}, page_content='Kafka: The Deﬁnitive\\nGuide\\nSECOND EDITION\\nReal-Time Data and Stream Processing at\\nScale\\nGwen Shapira, Todd Palino, Rajini\\nSivaram, and Krit Petty'),\n",
       " Document(metadata={'page': 22, 'source': './data/kafka_definite_guide.pdf'}, page_content='publish/subscribe messaging and why it is a critical\\ncomponent of data-driven applications. Publish/subscribe\\n(pub/sub) messaging  is a pattern that is characterized by\\nthe sender (publisher) of a piece of data (message) not\\nspeciﬁcally directing it to a receiver . Instead, the publisher\\nclassiﬁes the message somehow , and that receiver\\n(subscriber) subscribes to receive certain classes of\\nmessages. Pub/sub systems often have a broker , a central\\npoint where messages are published, to facilitate this\\npattern.\\nHow It Starts\\nMany use cases for publish/subscribe start out the same\\nway: with a simple message queue or interprocess\\ncommunication channel.  For example, you create an\\napplication that needs to send monitoring information\\nsomewhere, so you open a direct connection from your\\napplication to an app that displays your metrics on a\\ndashboard, and push metrics over that connection, as seen\\nin Figure 1-1 .'),\n",
       " Document(metadata={'page': 655, 'source': './data/kafka_definite_guide.pdf'}, page_content='infrastructure? What if there is a mistake and you need\\nto reprocess data?\\nUsability of APIs and ease of debugging\\nI’ve seen orders-of -magnitude diﬀerences in the time it\\ntakes to write a high-quality application among diﬀerent\\nversions of the same framework. Development time and\\ntime-to-market are important, so you need to choose a\\nsystem that makes you eﬀicient.\\nMakes hard things easy\\nAlmost every system will claim they can do advanced\\nwindowed aggregations and maintain local stores, but\\nthe question is: do they make it easy for you? Do they\\nhandle gritty details around scale and recovery , or do\\nthey supply leaky abstractions and make you handle\\nmost of the mess? The more a system exposes clean\\nAPIs and abstractions and handles the gritty details on\\nits own, the more productive developers will be.\\nCommunity\\nMost stream processing applications you consider are\\ngoing to be open source, and there’s no replacement for\\na vibrant and active community . Good community means\\nyou get new and exciting features on a regular basis, the\\nquality is relatively good (no one wants to work on bad\\nsoftware), bugs get ﬁxed quickly , and user questions get\\nanswers in a timely manner . It also means that if you get\\na strange error and Google it, you will ﬁnd information\\nabout it because other people are using this system and\\nseeing the same issues.\\nSummary'),\n",
       " Document(metadata={'page': 757, 'source': './data/kafka_definite_guide.pdf'}, page_content='send.buﬀer .bytes , Tuning MirrorMaker\\nrecord-error-rate metric , How Does the Idempotent\\nProducer W ork?, Overall producer metrics\\nrecord-retry-rate metric , Overall producer metrics\\nRecordMetadata object , Producer Overview , Sending a\\nMessage Synchronously\\nrecords\\ncommitting records and oﬀsets in a transaction ,\\nReading from a Kafka topic and writing to a database ,\\nReading data from a database, writing to Kafka, and\\nfrom there writing to another database\\ndeleting from a topic , Deleting Records from a T opic\\nheaders , Headers\\nrecords-lag-max metric , Fetch manager metrics , Lag\\nMonitoring\\nrecovery point objective (RPO) , Disaster recovery planning\\nrecovery time objective (R TO), Disaster recovery planning\\nredundant array of independent disks (RAID) , Disk\\nThroughput\\nregional and central clusters\\nin hub-and-spoke architecture , Hub-and-Spoke\\nArchitecture\\nuse of cross-cluster data mirroring , Use Cases of Cross-\\nCluster Mirroring\\nregular expressions')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How is weather today in Bangalore?\";\n",
    "\n",
    "aaa = retriever.invoke(query);\n",
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env-mistral",
   "language": "python",
   "name": "poetry-env-mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
