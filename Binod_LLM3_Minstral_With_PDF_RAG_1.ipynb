{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add pypdf\n",
    "# or pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./data/kafka_definite_guide.pdf')\n",
    "\n",
    "loaded_data = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vector DB from Chroma and Embedding from langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binod/Library/Caches/pypoetry/virtualenvs/langchain-mistral-pEPNAQNE-py3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vector DB. Proving PDF content and Embedding details.\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=loaded_data, \n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vector retriever for LLM \n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass three things to LLM\n",
    "\n",
    "# 1. Instruction or Prompt to LLM\n",
    "# 2. Context or Content as PDF chunks in splited\n",
    "# 3. Question or query\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Answer the question based only on the following context. If context is not available then you reply based on your information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping function for formating \n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM\n",
    "\n",
    "llm = ChatMistralAI(model_name=\"mistral-large-latest\", temperature=0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain function. It will take PDF content as LLM context, format those input\n",
    "# Question through invoke method\n",
    "# call Prompt variable\n",
    "# hit LLM\n",
    "# Show output\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apache Kafka is used for the following purposes:\n",
      "\n",
      "1. **Messaging System**: Kafka behaves as a message queue, providing ordering guarantees on the messages it stores. It allows clients to send messages to brokers, which then process and respond to these requests.\n",
      "\n",
      "2. **Data Streaming**: Kafka is used for building real-time data pipelines and streaming apps. It is a publish-subscribe messaging system that allows for high-throughput and low-latency data streaming.\n",
      "\n",
      "3. **Distributed Systems**: Kafka is used in distributed systems to facilitate communication between different components or microservices. It helps manage and route data between various parts of a distributed system.\n",
      "\n",
      "4. **Data Integration**: Kafka is used for integrating data from various sources and making it available to different targets. It can connect to external systems (for data import/export) through Kafka Connect and provide real-time data integration.\n",
      "\n",
      "5. **Log Aggregation**: Kafka can be used for log aggregation solutions to collect and store logs from multiple services and make them available in a standard format to multiple consumers.\n",
      "\n",
      "The context also mentions that Kafka has a robust ecosystem of tools and platforms that make running and using Kafka easier, including managed deployments, schema management, REST interfaces, and client library support. Additionally, it mentions that Kafka has a binary protocol over TCP for communication between clients and brokers."
     ]
    }
   ],
   "source": [
    "query= \"what is the use of Apache Kafka\"\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apache Kafka is a distributed event streaming platform that is widely used for building real-time data pipelines and streaming applications. Its primary uses include:\\n\\n1. **Real-Time Data Pipeline**:\\n   - **Data Ingestion**: Kafka can ingest large volumes of data from various sources such as databases, sensors, logs, and other applications.\\n   - **Data Processing**: It allows for real-time data processing using stream processing frameworks like Apache Flink, Apache Spark, or Kafka Streams.\\n   - **Data Storage**: Kafka can store data for a configurable amount of time, making it easy to replay and analyze historical data.\\n\\n2. **Event Streaming**:\\n   - **Publish-Subscribe Messaging**: Kafka supports a publish-subscribe messaging model, where producers publish data to topics, and consumers subscribe to those topics to read the data.\\n   - **Event-Driven Architectures**: It is often used to build event-driven applications where different components react to events in real-time.\\n\\n3. **Big Data Integration**:\\n   - **Data Integration**: Kafka can be used to integrate data from various sources into a unified data pipeline, which can then be processed and analyzed using big data tools like Hadoop, Spark, or Flink.\\n   - **Data Warehousing**: It can feed data into data warehouses or data lakes for long-term storage and analysis.\\n\\n4. **Log Aggregation**:\\n   - **Centralized Logging**: Kafka is often used to aggregate logs from multiple sources into a centralized logging system.\\n   - **Log Processing**: It can process logs in real-time to generate alerts, metrics, and other insights.\\n\\n5. **Metrics Collection**:\\n   - **Monitoring**: Kafka can be used to collect metrics from various systems and applications for real-time monitoring and alerting.\\n   - **Telemetry**: It can handle telemetry data from IoT devices and other sensors.\\n\\n6. **Microservices Communication**:\\n   - **Service-to-Service Communication**: Kafka is often used as a message broker for microservices architectures, enabling asynchronous communication between services.\\n   - **Event-Driven Microservices**: It can be used to build event-driven microservices that react to events in real-time.\\n\\n7. **Data Replication**:\\n   - **Geo-Replication**: Kafka can replicate data across different geographic locations, ensuring data availability and disaster recovery.\\n   - **Data Synchronization**: It can synchronize data between different systems and applications.\\n\\n8. **ETL (Extract, Transform, Load)**:\\n   - **Data Transformation**: Kafka can be used as part of an ETL pipeline to transform and load data into various data stores.\\n   - **Data Movement**: It can move data between different systems and applications efficiently.\\n\\n9. **Scalability and Performance**:\\n   - **High Throughput**: Kafka is designed to handle high throughput and low latency, making it suitable for real-time applications.\\n   - **Scalability**: It can scale horizontally by adding more brokers to the cluster, ensuring that it can handle increasing data volumes.\\n\\n10. **Fault Tolerance**:\\n    - **Data Durability**: Kafka provides durability by replicating data across multiple brokers, ensuring that data is not lost even if some brokers fail.\\n    - **High Availability**: It can be configured to provide high availability, ensuring that the system remains operational even in the event of failures.\\n\\nOverall, Apache Kafka is a versatile and powerful tool for handling real-time data streams, making it a popular choice for a wide range of applications and use cases.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call direct LLM without RAG for same question\n",
    "\n",
    "response = llm.invoke(\"what is the use of Apache Kafka\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env-mistral",
   "language": "python",
   "name": "poetry-env-mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
